# corpus_utils

##Word Space Visualization

This repository contains R scripts for visualizing word spaces based on the analysis of word frequency distributions. The visualization is designed to reveal thematic patterns within the distribution of words, especially named entities, assuming a Negative Binomial Distribution. The resulting plots provide insights into the semantic space of the analyzed collection of texts, aiding in the exploration of key themes and relationships among words. This can be useful when one needs to visualize keywords, or feature words, within a large collection of homogenious texts, such as medical articles, e-mails, news, reviews, or, for example an entire novel.

What is included:

**loader.py** Download and parse any Project Gutenberg book for analysis with **wordstats.py**

**compress.py**: Combines txt files into a "corpus", performs elementary parsing, and removes punctuation marks. The processed text is then saved for further analysis.

**wordstats.py**: Prepares word frequency lists. The results are saved to a text file.

**calculate.py**: Get some statistics from word frequencies.

**fit.R**: R script to estimate k and p parameters for NBD (Negative Binomial Distribution) assuming that word frequency distributions follow NBD

**plot.R**: R script to draw plots from data prepared by fit.R

**plot3D.R**: R script to draw 3D plots from data prepared by fit.R

**plot3D2.R**: Plot3D2.R offers improved visualisation of input data. It creates 3D scatterplot and allows for control of data filters. 

## Usage

### compress.py

Combine texts and prepare `corpus.txt`, the text corpus file:

bash
```
python ~/corpus_utils/compress.py /path/to/texts corpus.txt
```

This script takes text files from the specified directory, combines them, performs parsing, and removes punctuation marks.

### wordstats.py

Count word frequencies:

bash
```
python ~/corpus_utils/wordstats.py corpus.txt frequencies.txt 3 10000
```
Here, min word token frequency is 3, max word token frequency is 10000. For larger corpora it is recommended to set min frequency to a larger value.

### calculate.py

Now data is ready, we can calculate sum, mean, variance, and dispersion for each word-token. Input file, frequencies.txt, must be space-separated.

bash
```
python ~/corpus_utils/calculate.py frequencies.txt final_stats.txt
```
### fit.R

Using prepared data, estimate k and p parameters for NBD. Calculate sum for each word-token (absolute frequency in texts) and Document Frequency (DF). This is a compute-intensive operation and may take some time, especially for a larger dataset file. Run at your own risk.

bash
```
Rscript ~/corpus_utils/fit.R frequencies.txt results.tab

```
### plot.R

Plot.R creates scatterplot from data, which was prepared using fit.R 

bash
```
Rscript plot.R input.txt output.png 0.5
```

In this example, 0.5 is thresholp value of NBD parameters k and p. Try experimenting with this value to zoom in and out of the plot. Interesting values are between 0.1 and 0.7, see graphs in the repo.

This plot shows 2D visualisation of Melville's Moby Dick with such settings:

![Plot example 1](https://raw.githubusercontent.com/roverbird/corpus_utils/main/examples/graphs/moby05.png)

### plot3D.R


Plot3D.R creates 3D scatterplot from data, which was prepared using fit.R, where x axis is parameter k, y - parameter p, and z - Document Frequency (DF) The higher the DF, the more texts in the coprus contain the word. So, widespread words have higher DF value. This is not to be confused with absolute frequency of the word, measured by 'sum' in data prepared with fit.R

bash
```
Rscript plot3D.R input.txt output.png 20 0.1
```

In this example, **20** is rotation angle of the scatterplot, **0.1** is thresholp value of NBD parameters k and p. Try experimenting with this value to zoom in and out of the plot. Interesting values are between 0.1 and 0.7, see graphs in the repo.

### plot3D2.R

First, prepare data with fit2.R.
fit2.R is similar to fit2.R, but offers more functionality (calculated DF and other values).

bash
```
Rscript plot3D2.R input.txt output.png 300
```
Here, 300 is the projection angle. Try experimenting with this value to look around. Z-axis id Document Frequency (DF), the higher the value, the more individual texts in the corpus (your collection) contain the word. X and Y are NBD parameters induced from data generated by plot.R

Inside the script you can set filters! It comes handy to zoom in and out of your data or show a particular segment on the plot.
Filter explanation: 
- fr is Word Frequency 
- df is Document Frequency
- k and p are NBD parameters

Some examples to set filters:

- Values to map all data: k > 0 & p > 0 & fr > 1 & df > 1
- Values to map content background words: k > 5 & p > 0.8
- Values to map names of heros: k < 0.1 & p < 0.1 

Say, you have a collection of 500 texts. You only want to see data for tokens that meet conditions:

k > 0 & p > 0 & fr > 50 & df < 250 # Word frequency is over 50, and the word is met in half of texts. 

Example with k > 0 & p > 0 & fr > 50 & df < 500 for Russian Fairy Tales, image is zoomable.

![Plot example 2](https://raw.githubusercontent.com/roverbird/corpus_utils/main/examples/graphs/tales3Dver2.png)

### Add-ons

There are directories with addons. 
**texts**: this is how individual text can look before you combine them in to a corpus file with compress.py 
**examples/corpora**: a readily prepared corpus of fairy tales, you can run wordstats.py, fit.R and plot.R on it
**examples/graphs**: graphs from fairy tales corpora generated by plot.R

### Research Publication

The scripts in this repository reproduce the following research:

**Title:** [The Negative Binomial Model of Word Usage](http://siba-ese.unisalento.it/index.php/ejasa/article/view/12119)
**Authors:** Nina Alexeyeva, Alexandre Sotov
**Abstract:** How people make texts is a rarely asked but central question in linguistics. From the language engineering perspective texts are outcomes of stochastic processes. A cognitive approach in linguistics holds that the speakerâ€™s intention is the key to text formation. We propose a biologically inspired statistical formulation of word usage that brings together these views. We have observed that in several multilingual text collections word frequency distributions in a majority of non-rare words fit the negative binomial distribution (NBD). However, word counts in artificially randomized (permuted) texts agree with the geometric distribution. We conclude that the parameters of NBD deal with linguistic features of words. Specifically, named entities tend to have similar parameter values. We suggest that the NDB in natural texts accounts for intentionality of the speaker.

**DOI Code:** 10.1285/i20705948v6n1p84

**Keywords:** word frequency; negative binomial distribution (NBD); text randomization

